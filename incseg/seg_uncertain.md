# Segregation Inference

Statistical inference for segregation measures has been a topic of interest in the field for
decades, but remains a thorny analytical challenge [@Massey1978]. In recent decades,
methods have been developed for estimating appropriate statistics, relying on computational
approaches to overcome issues such as the lack of a reference distribution against which to test an
observed statistic, and given the continuous rise in computational power, several techniques have
become ripe for application toward large, urban datasets. 

## Single-Value Inference

Two key questions in the literature regarding single-value inference for segregation indices are (1)
how to construct an appropriate null hypothesis against which to test, and (2) whether that test
carries important substantive meaning. In the context of residential segregation, the first issue
arises because of the nature of zonal data used to calculate the measures. Unlike many conventional
empirical situations, a segregation index can still display wide variation in values despite a fixed
allocation of population in space, depending on how that population is aggregated into zones. A
classic example is given by the Dissimilarity index in the case of a city with a 50% population
split, which can yield measurements ranging from zero segregation to perfect segregation, depending
on how individuals are grouped into spatial aggregates. The sensitivity of many segregation indices
to arbitrary spatial boundaries is a well-known problem [@Reardon2004], which, in empirical
applications, means that segregation measures calculated from census data may be artificially
inflated or deflated simply because of the mix of zonal configuration and population group sizes.

An early discussion of the second issue is given by @Cortese1976, who advocate the utility of a
"standardized D" index, arguing that a randomly-allocated population follows a hypergeometric
distribution, and using that property to justify the theoretical grounds for a new measure akin to a
Z-score. In reply, @Massey1978 [p.587] argues that because a particular segregation index "lies some
number of standard deviations above expectation does not imply any clear social meaning. Rather, it
is the departure from evenness that bears the social and economic ramifications that make
residential segregation important and interesting". Following, Massey shows that in the context of
residential segregation, which typically relies on census data, "except for very atypical
situations, if an index... is large enough to be of substantive interest, the expected value and
variance will be of such a low order of magnitude that its statistical significance is naturally
assured".

Despite this critique, scholars have nonetheless continued developing computational approaches for
simulating evenly-distributed populations and testing whether an observed segregation measure
differs from a distribution of measures generated by these simulations. These methods are typically
demonstrated in settings such as occupational or educational segregation, where they avoid Massey's
criticisms of census tracts, but encounter other issues such like small unit sizes
[@Rathelot_2012]. One such approach is the bootstrap method [@Efron1979; @Davison1997], which uses
repeated resampling (with replacement) of the input data to create a synthetic dataset upon which a
segregation index is calculated. After many iterations, these indices from synthetic data can serve
as a reference distribution suitable for common frequentist statistics such as the one or two-sample
t-test. For segregation analysis, @davidson_reliable_2009 [p. 36] develops a bootstrap-based test
for the Gini index, showing that outside certain conditions, "unless the tails are very heavy
indeed, or the Gini index itself large, the bootstrap can yield acceptably reliable inference." The
same technique is generally applicable to other segregation indices, and @Allen2015a develop a
bias-corrected Dissimilarity index, using a series of simulation studies to confirm that the
bootstrap estimate achieves reasonably unbiased performance.

## Comparative Inference

Apart from the significance of a single segregation measure, a useful extension of inferential
statistics in segregation scholarship is analyzing the difference in segregation between two
entities--generally either two separate locations or a single location at two points in time. In the
economic geography literature, @Rey2010a introduce an alternative approach for simulating a null
distribution known as "random labelling." In their approach, the geographic units of two entities
(e.g. the U.S. and Mexico) are pooled, then each unit is re-assigned to an entity at random and a
test statistic (such as a segregation index) is calculated. Repeating this process many times is
akin to a jackknife procedure, yielding a reference distribution that can be used to estimate the
mean and standard deviation of the segregation measure.

In the bootstrapping case, distributions of segregation indices are generated for each entity using
simulated data, then these distributions are compared with one another using a two-sample
difference-in-means test. In the random-labelling case, a synthetic dataset is created for each
region, and the *difference between segregation statistics* of the two regions forms the reference
distribution for a one-sample means test. Thus while the two approaches share similar randomization
strategies and statistical foundations, they perform slightly different tests. The
comparative bootstrap approach uses a two-sample t-test to assess whether the segregation indices
from two different entities are equal to one another. The random labelling approach uses a
one-sample test to assess whether the observed difference between two entities is equal to the mean
of a distribution of simulated differences.

Since the latter approach was developed in the context of spatiotemporal analysis, a particular
benefit is that if the two entities have the same spatial footprint (i.e. the size and configuration
of the geographic units are the same, as may be the case for testing a single place at two points in
time), then planar enforcement is not violated. In other words, because the random labelling
approach was designed for the specific case of testing spatial autocorrelation, its randomization
strategy is capable of generating variation in values across entities, while preserving spatial
relationships of geographic units within each entity. As a result, the random labelling approach is
suitable for testing spatial segregation indices when geographic inputs for two entities are the
same[^1].

Despite the increasing computational feasibility of these techniques, exceedingly few empirical
examples have been published. One notable recent example is [@Liu2021a], which leverages a newly
developed software package that facilitates both single-value and comparative inference using a
variety of randomization strategies [@Cortes2020]. With these tools so readily available, we argue
there is fertile ground for exploring the evolution of residential segregation processes through an
inferential lens. While we recognize the distinction between substantive and statistical
significance, we believe that, particularly in the case of comparative inference, there are many
empirical questions that remain unexplored (e.g. whether the increase or decrease in segregation is
significantly greater in some regions of a country than others). Furthermore, the statistical issues
at hand become more complex when using data such as the U.S. Census Bureau's American Community
Survey which often carry high error margins at small geographic levels [@Reardon2018]. These issues
may well be exacerbated by the impending changes to "differential privacy" being considered at the
Census Bureau [@StevenRuggles2021].


# Discussion


# Conclusion


[^1]: For a further discussion, see @Rey2010a. There, the authors examine repeated measurements for the same geographic units (counties) in each entity (USA and Mexico), and their random labelling strategy is applied to the temporal index (i.e. randomizing values across temporal observations for the same geographic units) rather than the spatial index. In such case, observations are effectively randomized *in time* (rather than across space), leaving spatial relationships intact, and thus still testable. In our approach, randomization may occur in either dimension, yielding certain spatial indices inviable. The distinction boils down to the difference between jackknife and bootstrap estimation procedures, with the former using random sampling *without* replacement and the latter using sampling *with* replacement. In the context of geographic data, bootstrap sampling means that the simulated dataset may have observations that refer to the same geographic location while other existing locations may have no data. 